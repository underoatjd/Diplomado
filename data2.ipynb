{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENTREGABLE PROFESIONALES\n",
    "\n",
    "Objetivo: Desarrollar en el alumno las competencias de entender, manipular, administrar y analizar datos mediante la creación de informes para la toma de decisiones.\n",
    "\n",
    "\n",
    "Consideraciones:\n",
    "* Creación de base de datos en Excel, Access o SQL. (La base de datos puede ser real o ficticia)\n",
    "\n",
    "* Creación de un sistema de informes en Excel, Power BI o Looker Studio no menor a 6 vistas, en cada vista no menor a 5 objetos de visualización.\n",
    "\n",
    "* Realizar un análisis sobre el informe creado, desarrollando puntos de análisis para la toma de decisiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propuesta de un Análisis de datos y Modelo de IA para la Prevención del Abuso Sexual Infantil\n",
    "\n",
    "Introducción:\n",
    "El abuso sexual infantil es una problemática global que afecta a millones de menores de edad cada año. A pesar de los esfuerzos por mitigar este flagelo, sigue siendo un desafío identificar de manera temprana los factores de riesgo y prevenir los casos antes de que ocurran. La inteligencia artificial (IA) nos ofrece una herramienta innovadora y poderosa para hacer frente a este reto, utilizando la recolección de datos de manera ética y controlada para proteger a los más vulnerables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para verificar que el modelo funciona correctamente procederemos a crear una base de datos ficticios aleatorios, esto solo con el fin de solo realizar la validación y funcionalidad del programa.\n",
    "\n",
    "Debemos realizar 2 tipos de análisis de datos diferentes, el primero será un análisis de datos enfocado en un proceso ETL para un modelo de regresión logistica de Python, el segundo será un análisis de datos enfocado en un contexto mas grafico para poder realizar un dashboard para presentar estos datos con PowerBI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Instalamos la libreria de faker para poder generar datos random\n",
    "pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Importamos las librerias que vamos a utilizar.\n",
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Inicializar Faker para generar datos ficticios\n",
    "fake = Faker()\n",
    "\n",
    "# 4 aqui declaramos el numero de filas que vamos a utilizar para nuestra base.\n",
    "num_rows = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Generar datos aleatorios para cada columna utilizando un array de datos aleatorios, \n",
    "# usamos faker y usamos num_rows para la dantidad de row del dataframe en este caso usaremos 10.000 filas de datos\n",
    "\n",
    "# Verificar si el archivo CSV ya existe\n",
    "if os.path.exists(\"dataframe.csv\"):\n",
    "    # Cargar el archivo CSV en un DataFrame\n",
    "    dataframe = pd.read_csv(\"dataframe.csv\")\n",
    "else:\n",
    "    data = {\n",
    "    \"Edad\": np.random.randint(13, 18, num_rows),\n",
    "    \"Edad Victima\": np.random.randint(7, 14, num_rows),\n",
    "    \"Sexo\": [random.choice([\"Masculino\", \"Femenino\"]) for _ in range(num_rows)],\n",
    "    \"Año agresión\": np.random.randint(2015,2025, num_rows),\n",
    "    \"Lugar de residencia\": [random.choice([\n",
    "        \"Antonio Narino\", \"Barrios Unidos\", \"Bosa\", \"Chapinero\", \n",
    "        \"Ciudad Bolivar\", \"Engativa\", \"Fontibon\", \"Kennedy\", \n",
    "        \"La Candelaria\", \"Los Martires\", \"Puente Aranda\", \n",
    "        \"Rafael Uribe Uribe\", \"San Cristobal\", \"Santa Fe\", \n",
    "        \"Suba\", \"Sumapaz\", \"Teusaquillo\", \"Tunjuelito\", \n",
    "        \"Usaquen\", \"Usme\"]) for _ in range(num_rows)],\n",
    "    \n",
    "    \"Composicion del hogar\": [random.choice([\"Nuclear\", \"Monoparental\", \"Extendida\"]) for _ in range(num_rows)],\n",
    "    \"Grado de consanguinidad de persona a cargo\": [random.choice([\"Padres\", \"Hermanos\", \"Tios\", \"Abuelos\", \"Otros familiares\"]) for _ in range(num_rows)],\n",
    "    \"Numero de personas con quienes vive\": np.random.randint(1, 8, num_rows),\n",
    "    \"Profesion persona a cargo\": [random.choice([\n",
    "        \"Agricultor\", \"Empleado\",\"Profesional\",\"Obrero\",\"Desempleado\", \n",
    "        \"Independiente\",\"Ambulante\"]) for _ in range(num_rows)],\n",
    "    \n",
    "    \"Nivel escolar\": [random.choice([\"Primaria\", \"Secundaria\", \"Tecnico\", \"Universitario\"]) for _ in range(num_rows)],\n",
    "    \"Rendimiento academico promedio\": np.random.randint(1, 10, num_rows),\n",
    "    \"Antecedentes de maltrato familiar o escolar\": [random.choice([\"Si\", \"No\"]) for _ in range(num_rows)],\n",
    "    \"Participa en actividades extracurriculares\": [random.choice([\"Si\", \"No\"]) for _ in range(num_rows)],\n",
    "    \n",
    "    \"Usa redes sociales activamente\": [random.choice([\"Si\", \"No\"]) for _ in range(num_rows)],\n",
    "    \"Contacto de forma inapropiada en redes\": [random.choice([\"Si\", \"No\"]) for _ in range(num_rows)],\n",
    "    \n",
    "    \"Involucrado en otros incidentes de agresion\": [random.choice([\"Si\", \"No\"]) for _ in range(num_rows)],\n",
    "    \"Conocia a la victima\": [random.choice([\"Si\", \"No\"]) for _ in range(num_rows)],\n",
    "    \"Antecedentes penales o disciplinarios\": [random.choice([\"Si\", \"No\"]) for _ in range(num_rows)],\n",
    "    \n",
    "    \"Trastorno psicologico o adiccion\": [random.choice([\"Si\", \"No\"]) for _ in range(num_rows)],\n",
    "    \"Atencion psicologica previa\": [random.choice([\"Si\", \"No\"]) for _ in range(num_rows)],\n",
    "    \"Se siente arrepentido\": [random.choice([\"Si\", \"No\"]) for _ in range(num_rows)],\n",
    "    \"Reconoce que su comportamiento fue incorrecto\": [random.choice([\"Si\", \"No\"]) for _ in range(num_rows)],\n",
    "    \n",
    "    \"Premeditado o impulsivo\": [random.choice([\"Premeditado\", \"Impulsivo\"]) for _ in range(num_rows)],\n",
    "    \"Violencia fisica o amenazas\": [random.choice([\"Si\", \"No\"]) for _ in range(num_rows)],\n",
    "    \"Actuo solo o con otras personas\": [random.choice([\"Solo\", \"Con otras personas\"]) for _ in range(num_rows)],\n",
    "    \n",
    "    \"Edad inicio consumo pornografico\": np.random.randint(6, 15, num_rows),\n",
    "    \"Contenido violento o explicito en material\": [random.choice([\"Si\", \"No\"]) for _ in range(num_rows)],\n",
    "    \"Agresion sexual Confirmada\": [random.choice([\"Si\", \"No\"]) for _ in range(num_rows)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 Utilizamos pandas para crear el dataframe con el array de datos\n",
    "# Verificamos que los datos funcionen como dataframe y damos un vistazo a las primeras 5 filas de datos\n",
    "if os.path.exists(\"dataframe.csv\"):\n",
    "    dataframe.head(5)\n",
    "else:\n",
    "    dataframe = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metodología: \n",
    "\n",
    "* Utilizar metodología ETL\n",
    "\n",
    "Metodología de ETL con Python\n",
    "ETL (Extract, Transform, Load) es un proceso fundamental en el manejo de datos, donde se extraen datos de una fuente, se transforman para adecuarlos a un formato deseado y se cargan en un destino final (como bases de datos, sistemas de análisis o almacenes de datos). Este flujo garantiza que los datos sean consistentes, limpios y listos para ser analizados.\n",
    "\n",
    "¿Por qué es importante el ETL?\n",
    "* Integración de datos: Permite consolidar datos provenientes de múltiples fuentes en un único repositorio.\n",
    "* Mejora de calidad: Durante la transformación, se eliminan inconsistencias, duplicados y valores erróneos.\n",
    "* Toma de decisiones eficiente: Datos limpios y organizados aseguran análisis confiables para la toma de decisiones.\n",
    "* Automatización y escalabilidad: Automatiza procesos rutinarios de preparación de datos, optimizando recursos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como lo mencione anteriormente, realizaremos dos procesos diferentes, un proceso ETL para Python y otro para PowerBI, en este orden de ideas de aquí en adelante realizare la conversión de datos para el modelo que necesitamos. Además de esto debemos realizar unos pasos adicionales para validar que cada vez que corramos el programa no se recree la base de datos nuevamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 exportamos la base de datos a un documento csv llamado dataframe de aqui en adelante\n",
    "if os.path.exists(\"dataframe.csv\"):\n",
    "    dataframe.head(5)\n",
    "else:\n",
    "    dataframe.to_csv(\"dataframe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python y PowerBI\n",
    "\n",
    "De aquí en adelante solo utilizare la transformación de datos en Python para poder realizar el correcto ajuste del modelo que queremos utilizar e implementar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 Metodología de ETL con Python\n",
    "# Primero que todo, creamos una funcion que pueda transformar todos los datos si en 1 y los no en 0\n",
    "# Identificar columnas con valores \"Si\" / \"No\"\n",
    "cols_si_no = dataframe.columns[dataframe.isin(['Si', 'No']).any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 Aplicar la transformación a binario: \"Si\" a 1, \"No\" a 0\n",
    "dataframe[cols_si_no] = dataframe[cols_si_no].applymap(lambda x: 1 if x == 'Si' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 validamos que nuestro proceso de limpieza este funcionando.\n",
    "modelDataframe = dataframe\n",
    "modelDataframe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11 Debemos continuar con los procedimientos para la transformacion de los datos, usaremos \n",
    "# Ahora podemos revisar si los datos estan completos.\n",
    "modelDataframe.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 Como nosotros mismos realizamos la generación de la base de datos, sabemos que no hay datos nulos.\n",
    "# Continuaremos con la limpieza de los datos para nuestro modelo de regresión\n",
    "# Debemos realizar la conversión de datos de las variables categóricas nominales y ordinales si es el caso y existen, \n",
    "# aunque en este modelo solo tenemos variables categóricas nominales. Usaremos el get dummies de pandas para la transformacion.\n",
    "modelDataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13 Identificamos que hay 8 variables, o bueno mejor dicho 8 columnas que utilizan datos de variables nominales.\n",
    "# Tomemos estas 8 columnas y las transformamos\n",
    "dummies = pd.get_dummies(modelDataframe[[\n",
    "    \"Sexo\",\n",
    "    \"Lugar de residencia\",\n",
    "    \"Composicion del hogar\",\n",
    "    \"Grado de consanguinidad de persona a cargo\",\n",
    "    \"Profesion persona a cargo\",\n",
    "    \"Nivel escolar\",\n",
    "    \"Premeditado o impulsivo\",\n",
    "    \"Actuo solo o con otras personas\"\n",
    "    ]], dtype=int)\n",
    "\n",
    "print(dummies.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14 Una vez realizada la transformación de los dummies a datos binarios, procedemos a borrar los datos de nuestra variable modelDataframe \n",
    "# y después procedemos a unir los dummies a nuestra variable.\n",
    "modelDataframeLimpio = modelDataframe.drop([\n",
    "    \"Sexo\",\n",
    "    \"Lugar de residencia\",\n",
    "    \"Composicion del hogar\",\n",
    "    \"Grado de consanguinidad de persona a cargo\",\n",
    "    \"Profesion persona a cargo\",\n",
    "    \"Nivel escolar\",\n",
    "    \"Premeditado o impulsivo\",\n",
    "    \"Actuo solo o con otras personas\"\n",
    "], axis=1)\n",
    "\n",
    "print(modelDataframeLimpio.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 Una vez limpio el dataset de las variables categóricas, procederemos a unir las variables de los dummies con la función join de pandas.\n",
    "datasetModeloFinal = modelDataframeLimpio.join(dummies)\n",
    "print(datasetModeloFinal.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16 verificamos una vez mas nuestros datos con una descripcion.\n",
    "datasetModeloFinal.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17 Ahora podemos continuar con el analisis y verificamos las correlaciones con la funcion corr()\n",
    "datasetModeloFinal.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18 En este espacio, separaremos los datos.\n",
    "# En el eje X, dejamos todos los datos eliminando la columna [\"Agresion sexual Confirmada\"].\n",
    "# En el eje Y, solamente dejamos la columna [\"Agresion sexual Confirmada\"].\n",
    "x=datasetModeloFinal.drop([\"Agresion sexual Confirmada\"], axis=1)\n",
    "y=datasetModeloFinal[\"Agresion sexual Confirmada\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19 ahora importamos LinearRegression de sklearn para nuestros modelos\n",
    "# Importamos LinearRegression asi (pip install scikit-learn) \n",
    "# Desde Sklearn importamos train_test_split para dividir los datos en 2\n",
    "# Los datos que seran para entrenar el modelo y los datos para realizar la prueba.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_ent, X_pru, y_ent, y_pru = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 Desde Sklearn importaremos la función LogisticRegression.\n",
    "# Crearemos nuestro modelo\n",
    "# Le especificamos a nuestro modelo que de un máximo de 1000 iteraciones.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "modelo = LogisticRegression(max_iter=1000)\n",
    "modelo.fit(X_ent,y_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21 Ahora usamos nuestro conjunto de datos X de pruebas y lo guardamos en predicciones.\n",
    "predicciones = modelo.predict(X_pru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22 Ahora podemos verificar como le fue a nuestro modelo,\n",
    "# Si quedo bien entrenado o no.\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_pru, predicciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23 Podemos revisar algunas otras metricas\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_pru, predicciones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24 Esta matriz de confusiones, nos dará la claridad del porque tenemos varias métricas.\n",
    "# Puede que por sí solo nos dé un arreglo que no nos dice mucho.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_pru, predicciones)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25 Podemos mejorarlo un poco, agregándolo a un dataframe de pandas con columnas he índices.\n",
    "pd.DataFrame(cm, columns=[\"Predi : No\", \"Predi : Si\"], index=[\"Real : No\", \"Real : Si\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusiones:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
